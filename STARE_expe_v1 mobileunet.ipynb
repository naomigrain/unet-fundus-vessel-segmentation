{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4OJjwQsSEz3t"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import sklearn.metrics as sm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import backend as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc as sc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6r1zhFKHPhK",
        "outputId": "cd0d6f7d-67e3-4a05-bbcc-01ebeeb625c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/STARE/'\n",
        "TRAIN_PATH = PATH+'train/'\n",
        "TRAIN_AUG_PATH = TRAIN_PATH+'aug/'\n",
        "TRAIN_IMAGE_FOLDER = 'images'\n",
        "TRAIN_MASK_FOLDER = 'GT'\n",
        "IMAGE_PREFIX = 'image'\n",
        "MASK_PREFIX = 'mask'\n",
        "TRAIN_TARGET_SIZE = (256,256)\n",
        "TRAIN_BATCH_SIZE = 3\n",
        "SEED = 1\n",
        "TEST_PATH = PATH+'test/'\n",
        "TEST_IMAGE_PATH = TEST_PATH+'images/'\n",
        "SAVE_TEST_IMAGE_PATH = TEST_PATH+'pred/'\n",
        "TEST_TARGET_SIZE = (256,256)"
      ],
      "metadata": {
        "id": "75bKHmHhHRzN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjustData(img,mask,flag_multi_class,num_class):\n",
        "    if(flag_multi_class):\n",
        "        img /= 255\n",
        "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
        "        new_mask = np.zeros(mask.shape + (num_class,))\n",
        "        for i in range(num_class):\n",
        "            new_mask[mask == i,i] = 1\n",
        "        new_mask = np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
        "        mask = new_mask\n",
        "    elif(np.max(img) > 1):\n",
        "        img = img / 255\n",
        "        mask = mask /255\n",
        "        mask[mask > 0.5] = 1\n",
        "        mask[mask <= 0.5] = 0\n",
        "    return (img,mask)\n",
        "\n",
        "def trainGenerator(aug_dict, flag_multi_class = False, num_class = 2):\n",
        "    image_datagen = ImageDataGenerator(**aug_dict)\n",
        "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        TRAIN_PATH,\n",
        "        classes = [TRAIN_IMAGE_FOLDER],\n",
        "        class_mode = None,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size = TRAIN_TARGET_SIZE,\n",
        "        batch_size = TRAIN_BATCH_SIZE,\n",
        "        save_to_dir = TRAIN_AUG_PATH,\n",
        "        save_prefix  = IMAGE_PREFIX,\n",
        "        seed = SEED)\n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        TRAIN_PATH,\n",
        "        classes = [TRAIN_MASK_FOLDER],\n",
        "        class_mode = None,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size = TRAIN_TARGET_SIZE,\n",
        "        batch_size = TRAIN_BATCH_SIZE,\n",
        "        save_to_dir = TRAIN_AUG_PATH,\n",
        "        save_prefix  = MASK_PREFIX,\n",
        "        seed = SEED)\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    for (img,mask) in train_generator:\n",
        "        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "        yield (img,mask)"
      ],
      "metadata": {
        "id": "G67N1WmdH65Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen_args = dict(rotation_range=0.3,\n",
        "                    width_shift_range=0.05,\n",
        "                    height_shift_range=0.05,\n",
        "                    shear_range=0.1,\n",
        "                    horizontal_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "\n",
        "if not os.path.exists(TRAIN_PATH+'aug'):\n",
        "    os.makedirs(TRAIN_PATH+'aug')\n",
        "      \n",
        "data_gen = trainGenerator(aug_dict=data_gen_args)"
      ],
      "metadata": {
        "id": "q6O250YFIyUA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/iamyb/mobileunet"
      ],
      "metadata": {
        "id": "yI7UXfQEZsCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, SeparableConv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose\n",
        "from keras.layers import concatenate, Conv2D\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def mobileunet(pretrained_weights = None,input_size = (256,256,1)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    conv1  = SeparableConv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1  = BatchNormalization()(conv1)\n",
        "    conv1  = SeparableConv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1  = BatchNormalization()(conv1)\n",
        "    pool1  = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2  = SeparableConv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2  = BatchNormalization()(conv2)\n",
        "    conv2  = SeparableConv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2  = BatchNormalization()(conv2)\n",
        "    pool2  = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3  = SeparableConv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3  = BatchNormalization()(conv3)\n",
        "    conv3  = SeparableConv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    conv3  = BatchNormalization()(conv3)\n",
        "    pool3  = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    \n",
        "    conv4  = SeparableConv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4  = BatchNormalization()(conv4)\n",
        "    conv4  = SeparableConv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "    conv4  = BatchNormalization()(conv4)\n",
        "    pool4  = MaxPooling2D(pool_size=(2, 2))(conv4)    \n",
        "    \n",
        "    conv5  = SeparableConv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
        "    conv5  = BatchNormalization()(conv5)\n",
        "    conv5  = SeparableConv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
        "    conv5  = BatchNormalization()(conv5)\n",
        "    \n",
        "    conv6  = Conv2DTranspose(512, 3, strides=(2, 2), activation='relu', padding='same')(conv5)\n",
        "    cat6   = concatenate([conv4, conv6], axis = 3)\n",
        "    conv6  = SeparableConv2D(512, 3, activation='relu', padding='same')(cat6)\n",
        "    conv6  = BatchNormalization()(conv6)\n",
        "    conv6  = SeparableConv2D(512, 3, activation='relu', padding='same')(conv6)\n",
        "    conv6  = BatchNormalization()(conv6)\n",
        "    \n",
        "    conv7  = Conv2DTranspose(256, 3, strides=(2, 2), activation='relu', padding='same')(conv6)\n",
        "    cat7   = concatenate([conv3, conv7], axis = 3)\n",
        "    conv7  = SeparableConv2D(256, 3, activation='relu', padding='same')(cat7)\n",
        "    conv7  = BatchNormalization()(conv7)\n",
        "    conv7  = SeparableConv2D(256, 3, activation='relu', padding='same')(conv7)\n",
        "    conv7  = BatchNormalization()(conv7)\n",
        "    \n",
        "    conv8  = Conv2DTranspose(128, 3, strides=(2, 2), activation='relu', padding='same')(conv7)\n",
        "    cat8   = concatenate([conv2, conv8], axis = 3)\n",
        "    conv8  = SeparableConv2D(128, 3, activation='relu', padding='same')(cat8)\n",
        "    conv8  = BatchNormalization()(conv8)\n",
        "    conv8  = SeparableConv2D(128, 3, activation='relu', padding='same')(conv8)    \n",
        "    conv8  = BatchNormalization()(conv8)\n",
        "    \n",
        "    conv9  = Conv2DTranspose(64, 3, strides=(2, 2), activation='relu', padding='same')(conv8)\n",
        "    cat9   = concatenate([conv1, conv9], axis = 3)\n",
        "    conv9  = SeparableConv2D(64, 3, activation='relu', padding='same')(cat9)\n",
        "    conv9  = BatchNormalization()(conv9)\n",
        "    conv9  = SeparableConv2D(64, 3, activation='relu', padding='same')(conv9)        \n",
        "    conv9  = BatchNormalization()(conv9)\n",
        "    conv9  = Conv2D(2, 3, activation='relu', padding='same')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "    \n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "    model.compile(optimizer=Adam(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "u4_GNOpGS2eQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = mobileunet()\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('unet_STARE.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
        "callbacks = [ model_checkpoint ]\n",
        "history = model.fit(\n",
        "    data_gen,\n",
        "    steps_per_epoch=10,\n",
        "    epochs=80,\n",
        "    verbose=0,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ11qU9HJacL",
        "outputId": "4af2d142-0960-4140-f540-304c7280b41b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 images belonging to 1 classes.\n",
            "Found 10 images belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: loss improved from inf to 0.56768, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 2: loss improved from 0.56768 to 0.39824, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 3: loss improved from 0.39824 to 0.34251, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 4: loss improved from 0.34251 to 0.32711, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 5: loss improved from 0.32711 to 0.31625, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 6: loss improved from 0.31625 to 0.30289, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 7: loss improved from 0.30289 to 0.29684, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 8: loss improved from 0.29684 to 0.27390, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 9: loss improved from 0.27390 to 0.23416, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 10: loss improved from 0.23416 to 0.21385, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 11: loss improved from 0.21385 to 0.20035, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 12: loss improved from 0.20035 to 0.18021, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 13: loss improved from 0.18021 to 0.16989, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 14: loss improved from 0.16989 to 0.15658, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 15: loss improved from 0.15658 to 0.14556, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 16: loss improved from 0.14556 to 0.13798, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 17: loss improved from 0.13798 to 0.12679, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 18: loss improved from 0.12679 to 0.12588, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 19: loss improved from 0.12588 to 0.11895, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 20: loss improved from 0.11895 to 0.11414, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 21: loss improved from 0.11414 to 0.11001, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 22: loss improved from 0.11001 to 0.10942, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 23: loss improved from 0.10942 to 0.10768, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 24: loss improved from 0.10768 to 0.10671, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 25: loss improved from 0.10671 to 0.10350, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 26: loss improved from 0.10350 to 0.10183, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 27: loss improved from 0.10183 to 0.09992, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 28: loss did not improve from 0.09992\n",
            "\n",
            "Epoch 29: loss improved from 0.09992 to 0.09889, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 30: loss improved from 0.09889 to 0.09837, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 31: loss improved from 0.09837 to 0.09700, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 32: loss did not improve from 0.09700\n",
            "\n",
            "Epoch 33: loss improved from 0.09700 to 0.09254, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 34: loss did not improve from 0.09254\n",
            "\n",
            "Epoch 35: loss did not improve from 0.09254\n",
            "\n",
            "Epoch 36: loss did not improve from 0.09254\n",
            "\n",
            "Epoch 37: loss did not improve from 0.09254\n",
            "\n",
            "Epoch 38: loss improved from 0.09254 to 0.09217, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 39: loss improved from 0.09217 to 0.08970, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 40: loss improved from 0.08970 to 0.08552, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 41: loss improved from 0.08552 to 0.07951, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 42: loss improved from 0.07951 to 0.07901, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 43: loss improved from 0.07901 to 0.07807, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 44: loss improved from 0.07807 to 0.07670, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 45: loss improved from 0.07670 to 0.07500, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 46: loss did not improve from 0.07500\n",
            "\n",
            "Epoch 47: loss improved from 0.07500 to 0.07288, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 48: loss improved from 0.07288 to 0.07048, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 49: loss improved from 0.07048 to 0.06855, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 50: loss improved from 0.06855 to 0.06789, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 51: loss improved from 0.06789 to 0.06536, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 52: loss improved from 0.06536 to 0.06405, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 53: loss did not improve from 0.06405\n",
            "\n",
            "Epoch 54: loss did not improve from 0.06405\n",
            "\n",
            "Epoch 55: loss did not improve from 0.06405\n",
            "\n",
            "Epoch 56: loss improved from 0.06405 to 0.06303, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 57: loss improved from 0.06303 to 0.06260, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 58: loss improved from 0.06260 to 0.06176, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 59: loss improved from 0.06176 to 0.06163, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 60: loss improved from 0.06163 to 0.06010, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 61: loss improved from 0.06010 to 0.05946, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 62: loss did not improve from 0.05946\n",
            "\n",
            "Epoch 63: loss improved from 0.05946 to 0.05819, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 64: loss did not improve from 0.05819\n",
            "\n",
            "Epoch 65: loss did not improve from 0.05819\n",
            "\n",
            "Epoch 66: loss did not improve from 0.05819\n",
            "\n",
            "Epoch 67: loss improved from 0.05819 to 0.05658, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 68: loss did not improve from 0.05658\n",
            "\n",
            "Epoch 69: loss improved from 0.05658 to 0.05546, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 70: loss improved from 0.05546 to 0.05361, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 71: loss did not improve from 0.05361\n",
            "\n",
            "Epoch 72: loss did not improve from 0.05361\n",
            "\n",
            "Epoch 73: loss did not improve from 0.05361\n",
            "\n",
            "Epoch 74: loss did not improve from 0.05361\n",
            "\n",
            "Epoch 75: loss did not improve from 0.05361\n",
            "\n",
            "Epoch 76: loss improved from 0.05361 to 0.05283, saving model to unet_STARE.hdf5\n",
            "\n",
            "Epoch 77: loss did not improve from 0.05283\n",
            "\n",
            "Epoch 78: loss did not improve from 0.05283\n",
            "\n",
            "Epoch 79: loss did not improve from 0.05283\n",
            "\n",
            "Epoch 80: loss did not improve from 0.05283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GPU]\n",
        "start aug: 11.16\n",
        "train: 15 menit"
      ],
      "metadata": {
        "id": "M5_C7V3BI_Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "max_acc = max(acc)\n",
        "max_acc_i = acc.index(max_acc)\n",
        "loss_at_max_acc = loss[max_acc_i]\n",
        "\n",
        "print('acc:', max_acc, 'loss:', loss_at_max_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDjLbEhzNEii",
        "outputId": "cc1cafa8-6e09-46ae-9ced-c8ed90b86ab8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.9777533411979675 loss: 0.0528283417224884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14-02-2023; 23:40 <br>\n",
        "acc: 0.9706441164016724 loss: 0.09779315441846848 <br><br>\n",
        "\n",
        "15-02-2023; 01:53 <br>\n",
        "acc: 0.9781307578086853 loss: 0.06890685111284256 <br>\n",
        "change: remove zoom scale <br><br>\n",
        "\n",
        "15-02-2023; 10:16 <br>\n",
        "acc: 0.9769975543022156 loss: 0.07307402044534683 <br><br>\n",
        "\n",
        "16-02-2023; 01:44 <br>\n",
        "chabge: using mobileunet instead of pure unet <br>\n",
        "acc: 0.9777533411979675 loss: 0.0528283417224884 <br><br>\n"
      ],
      "metadata": {
        "id": "Mje4nO2tzAdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testGenerator(as_gray = True):\n",
        "    files=sorted(os.listdir(TEST_IMAGE_PATH))\n",
        "    num_image=len(files)\n",
        "    for i in range(num_image):\n",
        "        img = io.imread(os.path.join(TEST_IMAGE_PATH,files[i]),as_gray = as_gray)\n",
        "        print(files[i])\n",
        "        img = trans.resize(img,TEST_TARGET_SIZE)\n",
        "        img = np.reshape(img,img.shape+(1,))\n",
        "        img = np.reshape(img,(1,)+img.shape)\n",
        "        yield img"
      ],
      "metadata": {
        "id": "Xk2oXzs8NvUx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labelVisualize(num_class,color_dict,img):\n",
        "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
        "    img_out = np.zeros(img.shape + (3,))\n",
        "    for i in range(num_class):\n",
        "        img_out[img == i] = color_dict[i]\n",
        "      \n",
        "    return img_out\n",
        "\n",
        "def saveResult(npyfile):\n",
        "    files=os.listdir(TEST_IMAGE_PATH)\n",
        "    \n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = item[:,:,0]\n",
        "        img[img>0.1]=1\n",
        "        img[img<=0.1]=0\n",
        "        io.imsave(os.path.join(SAVE_TEST_IMAGE_PATH, files[i]+'_predict.png'),img)\n",
        "\n",
        "if not os.path.exists(SAVE_TEST_IMAGE_PATH):\n",
        "    os.makedirs(SAVE_TEST_IMAGE_PATH)"
      ],
      "metadata": {
        "id": "1At3uRQiN8Nc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_i = len(os.listdir(TEST_IMAGE_PATH))\n",
        "test_gen = testGenerator()\n",
        "results = model.predict_generator(test_gen,n_i,verbose=1)\n",
        "saveResult(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEarfTAnNtJJ",
        "outputId": "ea772ab2-f952-4535-94f2-9129c512b7b7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-046c9d728f71>:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  results = model.predict_generator(test_gen,n_i,verbose=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im0162.ppm\n",
            " 1/10 [==>...........................] - ETA: 4sim0163.ppm\n",
            " 2/10 [=====>........................] - ETA: 4sim0235.ppm\n",
            " 3/10 [========>.....................] - ETA: 3sim0236.ppm\n",
            " 4/10 [===========>..................] - ETA: 2sim0239.ppm\n",
            " 5/10 [==============>...............] - ETA: 2sim0240.ppm\n",
            " 6/10 [=================>............] - ETA: 1sim0255.ppm\n",
            " 7/10 [====================>.........] - ETA: 1sim0291.ppm\n",
            " 8/10 [=======================>......] - ETA: 0sim0319.ppm\n",
            " 9/10 [==========================>...] - ETA: 0sim0324.ppm\n",
            "10/10 [==============================] - 4s 368ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion_matrix_elements(groundtruth_list, predicted_list):\n",
        "    tn, fp, fn, tp = sm.confusion_matrix(groundtruth_list, predicted_list,labels=[0,1]).ravel()\n",
        "    tn, fp, fn, tp = np.float64(tn), np.float64(fp), np.float64(fn), np.float64(tp)\n",
        "\n",
        "    return tn, fp, fn, tp\n",
        "\n",
        "def get_prec_rec_IoU_accuracy(groundtruth_list, predicted_list):\n",
        "    tn, fp, fn, tp = get_confusion_matrix_elements(groundtruth_list, predicted_list)\n",
        "    \n",
        "    total = tp + fp + fn + tn\n",
        "    accuracy = (tp + tn) / total\n",
        "    prec = tp/(tp+fp)\n",
        "    rec = tp/(tp+fn)\n",
        "    IoU = tp/(tp+fp+fn)\n",
        "    \n",
        "    return prec,rec,IoU,accuracy\n",
        "\n",
        "def get_f1_score(groundtruth_list, predicted_list):\n",
        "    tn, fp, fn, tp = get_confusion_matrix_elements(groundtruth_list, predicted_list)\n",
        "    \n",
        "    f1_score = (2 * tp) / ((2 * tp) + fp + fn)\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "def get_validation_metrics(groundtruth,predicted):   \n",
        "    u,v = np.shape(groundtruth)\n",
        "    groundtruth_list = np.reshape(groundtruth,(u*v,))\n",
        "    predicted_list = np.reshape(predicted,(u*v,))\n",
        "    prec,rec,IoU,acc = get_prec_rec_IoU_accuracy(groundtruth_list, predicted_list)\n",
        "    f1_score = get_f1_score(groundtruth_list, predicted_list)\n",
        "\n",
        "    return prec,rec,IoU,acc,f1_score\n",
        "\n",
        "def evalResult(gth_path,npyfile,target_size=(512,512),flag_multi_class = False,num_class = 2):\n",
        "    files=sorted(os.listdir(gth_path))\n",
        "    print(files)\n",
        "    prec = 0\n",
        "    rec = 0\n",
        "    acc = 0\n",
        "    IoU = 0\n",
        "    f1_score=0\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = item[:,:,0]\n",
        "        gth = io.imread(os.path.join(gth_path,files[i]))\n",
        "        gth = trans.resize(gth,target_size)\n",
        "        img1=np.array(((img - np.min(img))/np.ptp(img))>0.1).astype(float)\n",
        "        gth1=np.array(((gth - np.min(gth))/np.ptp(gth))>0.1).astype(float)\n",
        "        p,r,I,a,f=get_validation_metrics(gth1,img1)\n",
        "        prec = prec+p\n",
        "        rec = rec+r\n",
        "        acc = acc+a\n",
        "        IoU = IoU+I\n",
        "        f1_score = f1_score+f\n",
        "    print(\"Precision=\",prec/(i+1), \"Recall=\",rec/(i+1), \"IoU=\",IoU/(i+1), \"acc=\",acc/(i+1), \"F1=\",f1_score/(i+1))    "
      ],
      "metadata": {
        "id": "7oEv5laHbUDh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_path = TEST_PATH + 'GT/'\n",
        "evalResult(gt_path,results)"
      ],
      "metadata": {
        "id": "byrJmAYQb5vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicted\n",
        "Precision= 0.7486418535745212 Recall= 0.8802602326917179 IoU= 0.6784592658474139 acc= 0.9618434906005859 F1= 0.8066515398308723 \n",
        "\n",
        "<b>15-02-2023; 10:30</b> \n",
        "Precision= 0.6528929903566489 Recall= 0.902777067006806 IoU= 0.6070497388289681 acc= 0.9461292266845703 F1= 0.7534104948419812\n",
        "\n",
        "<b>15-02-2023; 22:33</b> \n",
        "Precision= 0.09468648790927088 Recall= 1.0 IoU= 0.09468648790927088 acc= 0.09468994140625 F1= 0.17229373847296356"
      ],
      "metadata": {
        "id": "jEQo1J7scMJ2"
      }
    }
  ]
}